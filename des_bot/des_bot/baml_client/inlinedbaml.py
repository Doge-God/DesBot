# ----------------------------------------------------------------------------
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
# ----------------------------------------------------------------------------

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code using: baml-cli generate
# baml-cli is available with the baml package.

_file_map = {

    "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\n\nclient<llm> CustomGPT4o {\n  provider openai\n  options {\n    model \"gpt-4o\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomGPT4oMini {\n  provider openai\n  retry_policy Exponential\n  options {\n    model \"gpt-4o-mini\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> LocalMistral {\n  provider \"openai-generic\"\n  options {\n    base_url \"http://192.168.137.63:11434/v1\"\n    model \"mistral-small3.2-16k\"\n  }\n}\n\nclient<llm> LocalGemma270m {\n  provider \"openai-generic\"\n  options {\n    base_url \"http://192.168.137.63:11434/v1\"\n    model \"gemma3deterministic:270m\"\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/retry\nretry_policy Constant {\n  max_retries 3\n  // Strategy is optional\n  strategy {\n    type constant_delay\n    delay_ms 200\n  }\n}\n\nretry_policy Exponential {\n  max_retries 2\n  // Strategy is optional\n  strategy {\n    type exponential_backoff\n    delay_ms 300\n    multiplier 1.5\n    max_delay_ms 10000\n  }\n}",
    "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"python/pydantic\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.205.0\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode sync\n}\n",
    "minimal_chat.baml": "class Message {\n    role string\n    content string\n}\n\nclass ReplyTool {\n    tool_name \"reply\" @stream.done\n    response string \n}\n\nclass StopTool {\n    tool_name \"stop\" @description(#\"\n        when is a good time to stop the conversation\n    \"#) @stream.done\n    parting_message string @description(#\"\n        indicate to the user that the conversation is stopping\n    \"#)\n}\n\nclass QueryPastConversationTool {\n    tool_name \"search_past_conversation\" @stream.done\n    search_phrases string[] @description(#\"\n        phrases to search for in the past conversation with user\n    \"#)\n}\n\nfunction MinimalChatAgent(messages: Message[]) -> (ReplyTool | StopTool) {\n    client LocalMistral //CustomGPT4oMini //\"openai/gpt-4o\" // Set OPENAI_API_KEY to use this client.\n    prompt #\"\n        You are a minimal chat agent. You will respond to the messages provided.\n        If you think the conversation should stop, return a StopTool with a message.\n        Otherwise, return a ReplyTool with your response. If there is no proper tool \n        for complying with request, say so clearly\n\n        Messages:\n        {{ messages }}\n\n        {{ ctx.output_format }}\n    \"#\n}\n\ntest SanityCheck {\n    functions [MinimalChatAgent]\n    args {\n        messages [\n            {\n                role \"user\"\n                content \"Hello, how are you?\"\n            },\n            {\n                role \"assistant\"\n                content \"I'm doing well, thank you! How can I assist you today?\"\n            },\n            {\n                role \"user\"\n                content \"What is the weather like today?\"\n            }\n        ]\n    }\n}\n\ntest StopCheck {\n    functions [MinimalChatAgent]\n    args {\n        messages [\n            {\n                role \"user\"\n                content \"Hello, how are you?\"\n            },\n            {\n                role \"assistant\"\n                content \"I'm doing well, thank you! How can I assist you today?\"\n            },\n            {\n                role \"user\"\n                content \"Nothing, bye\"\n            }\n        ]\n    }\n    @@assert ( {{this.tool_name == \"stop\"}})\n}",
    "resume.baml": "// Defining a data model.\nclass Resume {\n  name string\n  email string\n  experience string[]\n  skills string[]\n}\n\n// Create a function to extract the resume from a string.\nfunction ExtractResume(resume: string) -> Resume {\n  // Specify a client as provider/model-name\n  // you can use custom LLM params with a custom client name from clients.baml like \"client CustomHaiku\"\n  client CustomGPT4oMini //\"openai/gpt-4o\" // Set OPENAI_API_KEY to use this client.\n  prompt #\"\n    Extract from this content:\n    {{ resume }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\n\n\n// Test the function with a sample resume. Open the VSCode playground to run this.\ntest vaibhav_resume {\n  functions [ExtractResume]\n  args {\n    resume #\"\n      Vaibhav Gupta\n      vbv@boundaryml.com\n\n      Experience:\n      - Founder at BoundaryML\n      - CV Engineer at Google\n      - CV Engineer at Microsoft\n\n      Skills:\n      - Rust\n      - C++\n    \"#\n  }\n}\n",
    "transcript_parse.baml": "\n\nfunction DetectTranscriptionEnd(transcription: string) -> bool {\n    client LocalMistral //CustomGPT4oMini //\"openai/gpt-4o\" // Set OPENAI_API_KEY to use this client.\n    prompt #\"\n        Is this a complete user message?\n\n        {{ transcription }}\n\n        {{ ctx.output_format }}\n    \"#\n}\n\ntest SanityCheck {\n    functions [DetectTranscriptionEnd]\n    args {\n        transcription \"Hello, how are you?\"\n    }\n    @@assert ( {{this == true}})\n}",
}

def get_baml_files():
    return _file_map