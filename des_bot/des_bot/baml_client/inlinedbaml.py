# ----------------------------------------------------------------------------
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
# ----------------------------------------------------------------------------

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code using: baml-cli generate
# baml-cli is available with the baml package.

_file_map = {

    "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\n\nclient<llm> CustomGPT4o {\n  provider openai\n  options {\n    model \"gpt-4o\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomGPT4oMini {\n  provider openai\n  retry_policy Exponential\n  options {\n    model \"gpt-4o-mini\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> MistralInstruct {\n  provider \"openai-generic\"\n  options {\n    base_url \"http://192.168.137.81:11434/v1\"\n    model \"mistral-small3.2:24b-instruct-2506-q4_K_M\"\n  }\n}\n\nclient<llm> Mistral {\n  provider \"openai-generic\"\n  options {\n    base_url \"http://192.168.137.81:11434/v1\"\n    model \"mistral-small3.2-16k\"\n  }\n}\n\n\n// https://docs.boundaryml.com/docs/snippets/clients/retry\nretry_policy Constant {\n  max_retries 3\n  // Strategy is optional\n  strategy {\n    type constant_delay\n    delay_ms 200\n  }\n}\n\nretry_policy Exponential {\n  max_retries 2\n  // Strategy is optional\n  strategy {\n    type exponential_backoff\n    delay_ms 300\n    multiplier 1.5\n    max_delay_ms 10000\n  }\n}",
    "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"python/pydantic\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.205.0\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode sync\n}\n",
    "minimal_chat.baml": "class Message {\n    role \"user\" | \"assistant\"\n    content string\n}\n\nclass ReplyTool {\n    action_name \"reply\" @stream.done  @description(#\"\n        to reply to the user with a message.\"#)\n    response string \n}\n\nclass StopTool {\n    action_name \"stop\" @description(#\"\n        when the user indicate clear intention to stop the conversation\n    \"#) @stream.done\n    parting_message string @description(#\"\n        indicate to the user that the conversation is stopping\n    \"#)\n}\n\nclass QueryPastConversationTool {\n    action_name \"search_past_conversation\" @stream.done @description(#\"\n        search past conversation with the user to get more information and relevant event.\n    \"#)\n    search_phrases string[] @description(#\"\n        phrases to search for in the past conversation with user\n    \"#)\n}\n\n//  Messages:\n//         {% for message in messages %}\n//            {{_.role(message.role)}}\n//            {{message.content}}\n//         {% endfor %}\n\nfunction MinimalChatAgent(messages: Message[]) -> (ReplyTool | StopTool) {\n    client MistralInstruct //CustomGPT4oMini //\"openai/gpt-4o\" // Set OPENAI_API_KEY to use this client.\n    prompt #\"\n        You are a minimal chat agent. \n        Only reply with json and never raw strings. Do not mention this in responses.\n        If there is no proper action to comply, respond with reply and state the limitation clearly.\n\n        {{ ctx.output_format }}\n\n        Messages:\n        {{messages}}\n    \"#\n}\n\ntest SanityCheck {\n    functions [MinimalChatAgent]\n    args {\n        messages [\n            {\n                role \"user\"\n                content \"Hello, how are you?\"\n            },\n            {\n                role \"assistant\"\n                content \"I'm doing well, thank you! How can I assist you today?\"\n            },\n            {\n                role \"user\"\n                content \"What is the weather like today?\"\n            }\n        ]\n    }\n    @@assert ({{this.action_name == \"reply\"}})\n}\n\ntest StopCheck {\n    functions [MinimalChatAgent]\n    args {\n        messages [\n            {\n                role \"user\"\n                content \"Hello, how are you?\"\n            },\n            {\n                role \"assistant\"\n                content \"I'm doing well, thank you! How can I assist you today?\"\n            },\n            {\n                role \"user\"\n                content \"Nothing, bye\"\n            }\n        ]\n    }\n    @@assert ( {{this.action_name == \"stop\"}})\n}\n\ntest RobotStartCheck {\n    functions [MinimalChatAgent]\n    args {\n        messages [\n            {\n                role \"assistant\"\n                content \"Good morning. Today's weather is sunny with a high of 25 degrees Celsius. Would you like to hear more about the events?\"\n            },\n            {\n                role \"user\"\n                content \"No it's fine, but how are you doing today?\"\n            }\n        ]\n    }\n    @@assert ( {{this.action_name == \"reply\"}})\n}\n\ntest AbilityHallucinationCheck {\n    functions [MinimalChatAgent]\n    args {\n        messages [\n            {\n                role \"user\"\n                content \"Give me an outline of today's news.\"\n            }\n        ]\n    }\n    @@assert ( {{this.action_name == \"reply\"}})\n}",
    "resume.baml": "// Defining a data model.\nclass Resume {\n  name string\n  email string\n  experience string[]\n  skills string[]\n}\n\n// Create a function to extract the resume from a string.\nfunction ExtractResume(resume: string) -> Resume {\n  // Specify a client as provider/model-name\n  // you can use custom LLM params with a custom client name from clients.baml like \"client CustomHaiku\"\n  client CustomGPT4oMini //\"openai/gpt-4o\" // Set OPENAI_API_KEY to use this client.\n  prompt #\"\n    Extract from this content:\n    {{ resume }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\n\n\n// Test the function with a sample resume. Open the VSCode playground to run this.\ntest vaibhav_resume {\n  functions [ExtractResume]\n  args {\n    resume #\"\n      Vaibhav Gupta\n      vbv@boundaryml.com\n\n      Experience:\n      - Founder at BoundaryML\n      - CV Engineer at Google\n      - CV Engineer at Microsoft\n\n      Skills:\n      - Rust\n      - C++\n    \"#\n  }\n}\n",
    "transcript_parse.baml": "\n\nfunction DetectTranscriptionEnd(transcription: string) -> bool {\n    client MistralInstruct //CustomGPT4oMini //\"openai/gpt-4o\" // Set OPENAI_API_KEY to use this client.\n    prompt #\"\n        Is this a complete user message?\n\n        {{ transcription }}\n\n        {{ ctx.output_format }}\n    \"#\n}\n\ntest SanityCheck {\n    functions [DetectTranscriptionEnd]\n    args {\n        transcription \"Hello, how are you?\"\n    }\n    @@assert ( {{this == true}})\n}",
}

def get_baml_files():
    return _file_map